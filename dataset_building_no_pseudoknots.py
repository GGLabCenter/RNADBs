# -*- coding: utf-8 -*-
"""dataset_building_no.pseudoknots.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15lpFmmrSeq4u-Wu-va3s6a3XrNrorJ3y
"""

import os 
import numpy as np
from random import shuffle
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

!git clone https://github.com/GGLabCenter/RNADBs/
!unzip RNADBs/download-dp-11178 -d data
input_dotbrackets = []
input_sequences = []
input_ids = []
len_limit = 700 # 2000 Ã¨ l'ideale

input_list = os.listdir('data')
shuffle(input_list)

np.random.seed(7)

for filename in input_list:
  sequence = []
  dotbracket = []
  num_lines = sum(1 for line in open('data/'+filename)
             if not line.startswith('# ') 
             and len(line)>0 
             and line != '\n')
  
  raw_file = open('data/'+filename).read().splitlines()
  
  for line in raw_file:
    if (not line.startswith('# ') and len(line)>0 and line != '\n'):
      if(len(sequence)>= (num_lines/2)):
        dotbracket.append(line)
      else:
        sequence.append(line)
  
  s = ''.join(sequence)
  d = ''.join(dotbracket)
  
  if(len(s)<len_limit):
    input_sequences.append(s)
    input_dotbrackets.append(d)
    input_ids.append(filename)
    
print("number of raw sequences in input: {}".format(len(input_sequences))) # total: 3554 sequences

token_index_seq = {}
token_index_db = {}

for sequence in input_sequences:
  for base in sequence:
    if base not in token_index_seq:
      token_index_seq[base] = len(token_index_seq)+1
      
for db_sequence in input_dotbrackets:
  for base in db_sequence:
    if base not in token_index_db:
      token_index_db[base] = len(token_index_db)+1

print("characters in the sequences dictionary (full): {}".format(len(token_index_seq)))
print("characters in the dotbrackets dictionary (full): {}".format(len(token_index_db)))
print()
sorted_by_value = sorted(token_index_seq.items(), key=lambda kv: kv[1])
print("dictionary for sequences")
print(sorted_by_value)
print()
sorted_by_value = sorted(token_index_db.items(), key=lambda kv: kv[1])
print("dictionary for dotbrackets")
print(sorted_by_value)

max_length = len(max(input_sequences, key=len))
print('Filtering now the dictionary in order to remove the seqs with the chars not allowed; allowed values are:')
allowed_dictionary_seq = ['A', 'C', 'G', 'U', 'M', 'N', 'R', 'W', 'S', 'Y', 'K', 'V', 'H', 'D', 'B']
print(allowed_dictionary_seq)
print()
input_sequences_temp = []
input_dotbrackets_temp = []
input_ids_temp = []
token_index_seq_temp = dict(token_index_seq)
#token_index_seq
print("token_index_seq:")
sorted_by_value = sorted(token_index_seq.items(), key=lambda kv: kv[1])
print(sorted_by_value)
#for val in allowed_dictionary_seq:
#  handleNull = token_index_seq.pop(val, None)
token_index_seq_final = {key: value+1 for (value, key) in enumerate(allowed_dictionary_seq)}
#token_index_seq['EOS'] = len(token_index_seq)
for char in token_index_seq_final.keys():
  if char not in allowed_dictionary_seq:
    handleNull = token_index_seq_final.pop(val, None)
print()
print("final dict for sequences:")
sorted_by_value = sorted(token_index_seq_final.items(), key=lambda kv: kv[1])

for sequence,db,fname in zip(input_sequences, input_dotbrackets, input_ids):
  copy_it = True
  for char in sequence:
    if char not in token_index_seq_final.keys():
      copy_it = False
  if copy_it == True:
    input_sequences_temp.append(sequence)
    input_dotbrackets_temp.append(db)
    input_ids_temp.append(fname)

input_sequences = input_sequences_temp
input_dotbrackets = input_dotbrackets_temp
input_ids = input_ids_temp






input_x = np.zeros((len(input_sequences), max_length, max(token_index_seq_final.values())+1))

for i, sequence in enumerate(input_sequences):
  for j, base in enumerate(sequence):
    index = token_index_seq_final.get(base)
    input_x[i, j, index] = 1
    if j == len(sequence)-1:
      if j != max_length-1:
        input_x[i, j+1:max_length, 0] = 1
        
input_y = np.zeros((len(input_dotbrackets), max_length, max(token_index_db.values()) + 1))

for i, sequence in enumerate(input_dotbrackets):
  for j, base in enumerate(sequence):
    index = token_index_db.get(base)
    input_y[i, j, index] = 1
    if j == len(sequence)-1:
      if j != max_length-1:
        input_y[i, j+1:max_length, 0] = 1
        
x_train, x_test, y_train, y_test = train_test_split(input_x, input_y, shuffle=True, test_size=50)

#training_set

#validation_set

#test_set

len(input_db)

token_index_seqlen = []

for i,sequence in enumerate(input_sequences):
  token_index_seqlen.append(len(sequence))

print("How is the distribution of the dataset in terms of length:")
print()

plt.figure()
plt.hist(token_index_seqlen,20) 

plt.xlabel("sequences' length", fontsize=16)  
plt.ylabel("sequences' number", fontsize=16)

input_x.shape

print(x_train.shape)
print(x_test.shape)
print(x_val.shape)
print(input_x.shape)

x_train, x_test, y_train, y_test = train_test_split(input_x, input_y, shuffle=True, test_size=50)
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, shuffle=True, test_size=0.20)

input_shape = x_train[0].shape
output_shape = y_train[0,0,:].shape[0]